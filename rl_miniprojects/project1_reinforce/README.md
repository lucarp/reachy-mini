# Project 1: REINFORCE (Policy Gradient)

**Learning Time**: 2 days
**Difficulty**: â­â­â˜†â˜†â˜† (Beginner)
**Algorithm Type**: On-policy, Model-free, Policy-based

---

## ðŸ“š Overview

REINFORCE is the simplest and most fundamental **policy gradient** algorithm in reinforcement learning. It directly optimizes the policy by following the gradient of expected return.

**Key Idea**: Instead of learning a value function (like Q-learning), REINFORCE learns a policy Ï€(a|s) that directly maps states to actions.

---

## ðŸŽ¯ Learning Objectives

After completing this project, you will understand:

1. âœ… **Policy Gradient Theorem** - The mathematical foundation of policy gradients
2. âœ… **Monte Carlo Returns** - How to compute cumulative rewards
3. âœ… **Baseline** - Why and how to reduce variance
4. âœ… **On-policy Learning** - Difference between on-policy and off-policy
5. âœ… **Episode-based Updates** - Training after full episode completion
6. âœ… **Stochastic Policies** - Why exploration naturally emerges from stochastic policies

---

## ðŸ“– Theory

### The Goal of RL

Find policy Ï€ that maximizes expected return:

```
J(Ï€) = E_{Ï„~Ï€} [ Î£_t Î³^t r_t ]
```

where Ï„ is a trajectory (sâ‚€, aâ‚€, râ‚, sâ‚, aâ‚, ...).

### Policy Gradient Theorem

The **policy gradient theorem** tells us how to improve the policy:

```
âˆ‡_Î¸ J(Î¸) = E_{Ï„~Ï€_Î¸} [ Î£_t âˆ‡_Î¸ log Ï€_Î¸(a_t | s_t) * G_t ]
```

**Intuition**:
- Increase probability of action `a_t` if it led to high return `G_t`
- Decrease probability if it led to low return
- The gradient âˆ‡log Ï€ tells us which direction to move parameters Î¸

### REINFORCE Algorithm (Williams, 1992)

```
Initialize policy parameters Î¸ randomly

for each episode:
    Generate episode Ï„ = (sâ‚€, aâ‚€, râ‚, ..., s_T) using Ï€_Î¸

    for each timestep t = 0, 1, ..., T-1:
        Compute return: G_t = Î£_{k=t}^T Î³^{k-t} * r_k

        Compute policy gradient:
            âˆ‡_Î¸ J â‰ˆ âˆ‡_Î¸ log Ï€_Î¸(a_t | s_t) * G_t

        Update parameters:
            Î¸ â† Î¸ + Î± * âˆ‡_Î¸ log Ï€_Î¸(a_t | s_t) * G_t
```

### Return Calculation

The **return** G_t is the discounted sum of future rewards starting from time t:

```
G_t = r_{t+1} + Î³ * r_{t+2} + Î³Â² * r_{t+3} + ...
    = r_{t+1} + Î³ * G_{t+1}
```

where Î³ âˆˆ [0, 1] is the discount factor.

**Example**:
- Rewards: [1, 2, 3, 4]
- Discount Î³ = 0.9
- Returns: [1 + 0.9*2 + 0.81*3 + 0.729*4 = 8.116, ...]

### Baseline (Variance Reduction)

Raw REINFORCE has **high variance**. We reduce it with a baseline b(s):

```
âˆ‡_Î¸ J â‰ˆ âˆ‡_Î¸ log Ï€_Î¸(a_t | s_t) * (G_t - b(s_t))
```

**Best baseline**: State value function V(s_t)

**Why it works**:
- If G_t > V(s_t): This action was better than expected â†’ increase probability
- If G_t < V(s_t): This action was worse than expected â†’ decrease probability

**Important**: Baseline does NOT introduce bias (proof in Sutton & Barto, ch. 13)

---

## ðŸ—ï¸ Architecture

### Policy Network

```
Input: State (6D)
  â†“
FC Layer (64 units) + ReLU
  â†“
FC Layer (64 units) + ReLU
  â†“
Split into two heads:
  â”œâ”€â†’ Mean Head â†’ Î¼(s)        [action mean]
  â””â”€â†’ Log Std Head â†’ log Ïƒ(s) [action std]
  â†“
Sample action: a ~ N(Î¼(s), Ïƒ(s))
```

### Value Network (Baseline)

```
Input: State (6D)
  â†“
FC Layer (64 units) + ReLU
  â†“
FC Layer (64 units) + ReLU
  â†“
Value Head â†’ V(s)  [scalar]
```

---

## ðŸ” Key Concepts Explained

### 1. Why Policy Gradients?

**Value-based methods** (like Q-learning):
- Learn Q(s, a) or V(s)
- Derive policy implicitly: Ï€(s) = argmax_a Q(s, a)
- âŒ Can only handle discrete actions
- âŒ Policy is deterministic (no exploration after convergence)

**Policy-based methods** (like REINFORCE):
- Learn Ï€(a|s) directly
- âœ… Can handle continuous actions
- âœ… Natural exploration (stochastic policy)
- âœ… Can learn stochastic optimal policies

### 2. Stochastic vs Deterministic Policies

**Deterministic**: Ï€(s) = a (always same action)

**Stochastic**: Ï€(a|s) = P(action = a | state = s) (probability distribution)

For continuous actions, we use **Gaussian policy**:
```
Ï€(a|s) = N(Î¼_Î¸(s), Ïƒ_Î¸(s))
```

### 3. On-Policy vs Off-Policy

**On-policy** (REINFORCE):
- Use data collected by current policy Ï€
- Throw away data after each update
- âŒ Sample inefficient
- âœ… Stable, simple

**Off-policy** (DQN, DDPG):
- Use data collected by old policies
- Store in replay buffer
- âœ… Sample efficient
- âŒ More complex, can be unstable

### 4. Why Wait for Full Episode?

REINFORCE is a **Monte Carlo** method:
- Needs full episode to compute returns G_t
- Cannot update after each step
- Works well for episodic tasks
- Not suitable for continuous/infinite-horizon tasks

---

## ðŸ’» Implementation Details

### Gaussian Policy

```python
# Forward pass
mean, std = policy_network(state)

# Sample action
dist = Normal(mean, std)
action = dist.sample()
log_prob = dist.log_prob(action).sum()

# Loss (policy gradient)
policy_loss = -(log_prob * (return - baseline)).mean()
```

### Return Normalization

Normalize returns to reduce variance:

```python
returns = (returns - returns.mean()) / (returns.std() + 1e-8)
```

This helps because:
- Neural networks work better with normalized inputs
- Prevents exploding/vanishing gradients

### Gradient Clipping

Clip gradients to prevent updates that are too large:

```python
torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)
```

---

## ðŸ“Š Hyperparameters

| Parameter | Value | Explanation |
|-----------|-------|-------------|
| `policy_lr` | 3e-4 | Policy learning rate (Adam) |
| `value_lr` | 1e-3 | Value learning rate (Adam) |
| `gamma` | 0.99 | Discount factor (future rewards) |
| `hidden_dim` | 64 | Neural network hidden layer size |

**Tuning Tips**:
- If training is unstable â†’ Lower learning rates
- If convergence is slow â†’ Increase learning rates (carefully)
- If agent is too short-sighted â†’ Increase gamma
- If agent never terminates â†’ Decrease gamma

---

## ðŸš€ Usage

### Training

```bash
# Basic training (500 episodes)
python train.py

# Custom parameters
python train.py --episodes 1000 --policy-lr 1e-4 --gamma 0.95

# With different random seed
python train.py --seed 123
```

### Evaluation

```bash
# Evaluate trained model
python test.py

# Compare with random baseline
python test.py --baseline

# Evaluate specific checkpoint
python test.py --checkpoint reinforce_epoch500.pt
```

---

## ðŸ“ˆ Expected Results

After 500 episodes of training:

- **Mean Reward**: -50 to -20 (higher is better)
- **Mean Tracking Distance**: 0.2 to 0.4 (lower is better)
- **Success Rate**: 60-80% (ball stays in frame)

**Comparison to Random**:
- Random policy: ~-150 reward, 0.8 distance
- REINFORCE: 3-5x improvement

**Learning Curve**:
- Episodes 0-100: High variance, slow improvement
- Episodes 100-300: Steady improvement
- Episodes 300-500: Convergence, low variance

---

## ðŸ› Common Issues

### 1. Training is unstable (reward oscillates wildly)

**Causes**:
- Learning rate too high
- No baseline (high variance)
- Returns not normalized

**Solutions**:
```bash
python train.py --policy-lr 1e-4 --value-lr 5e-4
```

### 2. Agent doesn't learn anything

**Causes**:
- Learning rate too low
- Discount factor too low (agent too short-sighted)
- Bad initialization

**Solutions**:
- Increase learning rates
- Increase gamma to 0.99
- Try different random seeds

### 3. Training is very slow

**Causes**:
- Episodes are too long (300 steps)
- Policy gradient has high variance

**Solutions**:
- This is expected! REINFORCE is sample-inefficient
- Project 2 (DQN) and Project 4 (PPO) will be faster

---

## ðŸ§  Study Questions

Test your understanding:

1. **Why does REINFORCE multiply gradient by return G_t?**
   <details>
   <summary>Answer</summary>
   To weight the gradient by how good the outcome was. If G_t is high, we want to increase probability of that action. If G_t is low, decrease it.
   </details>

2. **Why use a baseline? Does it introduce bias?**
   <details>
   <summary>Answer</summary>
   Baseline reduces variance in gradient estimates, making training more stable. It does NOT introduce bias because E[âˆ‡log Ï€(a|s) * b(s)] = 0.
   </details>

3. **Why can't REINFORCE use a replay buffer like DQN?**
   <details>
   <summary>Answer</summary>
   REINFORCE is on-policy - it must use data from current policy Ï€_Î¸. Old data from Ï€_Î¸_old is no longer valid after Î¸ changes. Off-policy methods like DQN use importance sampling to correct for this.
   </details>

4. **What's the difference between return and reward?**
   <details>
   <summary>Answer</summary>
   - **Reward** r_t: Immediate signal at one timestep
   - **Return** G_t: Cumulative discounted sum of all future rewards from timestep t onward
   </details>

5. **Why use a stochastic policy instead of deterministic?**
   <details>
   <summary>Answer</summary>
   - Natural exploration (built into policy)
   - Can learn stochastic optimal policies (e.g., rock-paper-scissors)
   - Gradient estimation works better with continuous probability distributions
   </details>

---

## ðŸ“š Further Reading

### Papers
- **REINFORCE**: Williams, R. J. (1992). "Simple statistical gradient-following algorithms for connectionist reinforcement learning."
- **Policy Gradient Methods**: Sutton et al. (1999). "Policy gradient methods for reinforcement learning with function approximation."

### Textbooks
- Sutton & Barto (2018), Chapter 13: "Policy Gradient Methods"
- Goodfellow, Bengio & Courville (2016), Chapter 20.3: "Policy Gradient"

### Online Resources
- [Spinning Up in Deep RL - Policy Gradients](https://spinningup.openai.com/en/latest/algorithms/vpg.html)
- [Lilian Weng's Policy Gradient Algorithms](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)

---

## âœ… Checklist

Mark off as you complete:

- [ ] Read through all theory sections
- [ ] Understand policy gradient theorem
- [ ] Run training script and observe learning
- [ ] Evaluate trained agent vs random baseline
- [ ] Experiment with hyperparameters (learning rate, gamma)
- [ ] Answer all study questions
- [ ] Can explain REINFORCE to someone else

---

## ðŸŽ¯ Next Steps

Once you've mastered REINFORCE, move on to:

**â†’ Project 2: DQN (Deep Q-Network)**
- Learn value-based methods
- Understand off-policy learning
- Use replay buffers for sample efficiency

This will give you a complete picture: policy-based (REINFORCE) vs value-based (DQN).
